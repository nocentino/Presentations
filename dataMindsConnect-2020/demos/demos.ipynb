{
    "metadata": {
        "kernelspec": {
            "name": "powershell",
            "display_name": "PowerShell"
        },
        "language_info": {
            "name": "powershell",
            "codemirror_mode": "shell",
            "mimetype": "text/x-sh",
            "file_extension": ".ps1"
        }
    },
    "nbformat_minor": 2,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "code",
            "source": [
                "kubectl config use-context kubernetes-admin@kubernetes-arc\n",
                "\n",
                "$ENV:AZDATA_USERNAME='arcadmin'\n",
                "$ENV:AZDATA_PASSWORD='S0methingS@Str0ng!'\n",
                "\n",
                "azdata login --endpoint https://192.168.13.131:30080 --namespace arc\n",
                "azdata arc postgres server create -n postgres01 --workers 0 --dev\n",
                "azdata arc sql mi create -n sqldemo01 --dev"
            ],
            "metadata": {
                "azdata_cell_guid": "93995c3e-1310-4366-b8f1-92eaa1e779b2",
                "tags": [
                    "hide_input"
                ]
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                " **First up, let's log into our Azure Arc Data Controller using the Azure Data CLI ( `azdata` )**"
            ],
            "metadata": {
                "azdata_cell_guid": "62cb517a-00c1-4765-9431-c680e32480af"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "$ENV:AZDATA_USERNAME='arcadmin'\n",
                "$ENV:AZDATA_PASSWORD='S0methingS@Str0ng!'\n",
                "\n",
                "azdata login --endpoint https://192.168.13.131:30080 --namespace arc"
            ],
            "metadata": {
                "azdata_cell_guid": "fc8c1305-3bbf-4260-b355-99001a9f310c",
                "tags": []
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "**As with any CLI tool, your first stop should be the help :) Get familiar with the syntax and options available.**"
            ],
            "metadata": {
                "azdata_cell_guid": "5d783509-9daf-4101-a131-540739d7a3fa"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "azdata arc --help"
            ],
            "metadata": {
                "azdata_cell_guid": "dbc40c6a-fbc7-4a59-837d-d510e86adb3c",
                "tags": []
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "**I've  already deployed one Managed Instance and one PostgreSQL Hyperscale instance into our Arc Data Services on premises environment. To get connection information we can use `azdata`.**"
            ],
            "metadata": {
                "azdata_cell_guid": "0ba40889-9e24-451c-a52d-5aefd1258b1f"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "azdata arc sql --help\n",
                "azdata arc sql mi list"
            ],
            "metadata": {
                "azdata_cell_guid": "07b0806e-eded-4acf-9550-dff676c50bd2",
                "tags": []
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "azdata arc postgres endpoint list --name postgres01 -o table"
            ],
            "metadata": {
                "azdata_cell_guid": "9f4695db-0d91-4849-b6af-9ce4e141a162"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "**Services in Arc enabled Data Services are implemented as Kubernetes Services.**  \n",
                "**We can use Kubernetes native tooling to interact with our Arc Enabled Data Services Instances and Configurations.**  \n",
                "**By default you get two Service Endpoints per Service, internal (ClusterIP) and External (NodePort)**"
            ],
            "metadata": {
                "azdata_cell_guid": "6f42afc3-35ac-410a-8314-0bc3ceb492b9"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "kubectl get services --namespace arc "
            ],
            "metadata": {
                "azdata_cell_guid": "b8e117ba-9e8c-4cd1-a2e7-ecb0e46dd4d5",
                "tags": []
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "**Let's get the Port (NodePort) for our deployed Managed Instance which is backed by the Service `sqldemo01-external-svc`**"
            ],
            "metadata": {
                "azdata_cell_guid": "4229429b-15fd-4bbe-8b5a-111fbdb79421"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "$NodePort=kubectl get services --namespace arc sqldemo01-external-svc -o jsonpath='{ .spec.ports[0].nodePort }'\n",
                "sqlcmd -S 192.168.13.131,$NodePort -U arcadmin -P 'S0methingS@Str0ng!' -Q 'SELECT @@VERSION'"
            ],
            "metadata": {
                "azdata_cell_guid": "a95dedc5-8463-4410-85b4-dd345590d47c",
                "tags": []
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "**Let's begin the process of deploying a Managed Instance using `azdata`. We'll start with checking out the help first...and check out some of the options available to us.**\n",
                "\n",
                "-   **Limits -** an upper limit for access to resources, CPU and Memory. \n",
                "-   **Requests -** guaranteed access to resources, CPU or Memory.\n",
                "-   **StorageClasses -** dynamically provisioned cluster storage attached into the Pod for Persistent Storage. By default, you will get a Data (includes transaction log) and a Log Persistent Volume.\n",
                "\n",
                "In disconnected mode, you can also deploy with Azure Data Studio.  GitOps, Portal, Azure CLI and PowerShell experiences are all on the way."
            ],
            "metadata": {
                "azdata_cell_guid": "940f3128-32e7-4ba5-8056-757bce69b92a"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "azdata arc sql mi create --help"
            ],
            "metadata": {
                "azdata_cell_guid": "5831fca7-8bea-496d-b3da-ad0a247aac35",
                "tags": []
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "**We can create one with minimal options which will get you a Managed Instance with no CPU or Memory limits and Requests of 100m and 100Mi and a single disk from your default StorageClass. Takes about 1 minute with containers pre-pulled.**"
            ],
            "metadata": {
                "azdata_cell_guid": "8e4f0bb9-5e98-4393-a20d-09ba372510fb"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "azdata arc sql mi create --name sqldemo02 --dev"
            ],
            "metadata": {
                "azdata_cell_guid": "9506ddae-7514-497b-95b1-05f38ff3979b",
                "tags": []
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "**Let's check out our newly deployed Managed Instance. Now we have two unique Service Endpoints.**"
            ],
            "metadata": {
                "azdata_cell_guid": "132bd1a6-00cc-4cf9-8e98-690577164ea2"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "azdata arc sql mi list "
            ],
            "metadata": {
                "azdata_cell_guid": "5822049d-ad60-42e7-82d6-f152c77dc657",
                "tags": []
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "**Now with all of that let's check out some of the management tooling and to do that...let's upload some data into Azure. You can use environment variables to help build automation around these tools. Otherwise the prompts are going to be interactive...asking for things like username, namespace, workspace\\_id and so on.**"
            ],
            "metadata": {
                "azdata_cell_guid": "f7a7c043-9826-4b82-b02b-7b9204b98328"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "$ENV:AZDATA_USERNAME=\"arcadmin\"\n",
                "$ENV:AZDATA_PASSWORD=\"S0methingS@Str0ng!\"\n",
                "\n",
                "azdata login --namespace arc\n",
                "\n",
                "\n",
                "$ENV:WORKSPACE_ID='31f660e3-cb32-4f1c-97f1-e69896eca90f'\n",
                "$ENV:SPN_CLIENT_ID='a7b906f4-297f-4aaa-821d-a410e398973e'\n",
                "$ENV:SPN_CLIENT_SECRET='2ec0a112-0d25-4168-81fd-9917f86cabe4'\n",
                "$ENV:SPN_TENANT_ID='f17cbd44-7697-453e-941c-efe0a4c2d55a'\n",
                "$ENV:WORKSPACE_SHARED_KEY='TNeKgNLo67T9+8dAN1oDdBzrdwiIdJJ/HD1dwbWRCuWIbyuET/SR/ZpUtkRQvkZxYxrYgB+9niNzXQxAZMQdkg=='\n",
                "$ENV:SPN_AUTHORITY='https://login.microsoftonline.com'\n",
                "\n",
                "azdata arc dc export --type metrics --path metrics.json --force\n",
                "azdata arc dc upload --path metrics.json\n",
                "\n",
                "azdata arc dc export --type logs --path logs.json --force\n",
                "azdata arc dc upload --path logs.json"
            ],
            "metadata": {
                "azdata_cell_guid": "080508d1-681c-487d-9923-e5a2354e02ea"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "**Management, Monitoring and Logging** - our implementation of Azure Arc Enabled Data Services is disconnected. The local tools hold the metrics, logging and usage data. One uploaded, you can use tools in Azure to analyze that data. Let's look at the local tools then hop into Azure to look at the tools available there.\n",
                "\n",
                "1.  Right click Manage to Launch the **Azure Arc Data Controller Dashboard**\n",
                "    * Click on the Managed Instance **sqldemo01** to launch the management dashboard\n",
                "    * To view logs on-prem click Kibana Dashboard\n",
                "        * Set the date range to 15 days ago and search for **master** and **tempdb**\n",
                "    * To view Metrics on-prem click Grafana Dashboard\n",
                "        * Review the core SQL Server metrics\n",
                "        \n",
                "1. Back on the main **Azure Arc Data Controller Dashboard**, notice the Connection mode of the Data Controller: **Indirect**. Click on **Open in Azure Portal**.    \n",
                "1. Click on the Resource Group name in our case it's **arc-onprem**\n",
                "1. Check out the resources in the Portal\n",
                "    * Click on Metrics, change the **Metric Namespace** to ```sql server``` **change the Metric** to ```Memory Usage```. change the date range to 24 hours.\n",
                "1. In the Log Analytics workspace and run the following query\n",
                "    -   ```\n",
                "         sqlManagedInstances_logs_CL |  where TimeGenerated > ago(7d)  |  order by TimeGenerated desc \n",
                "        ```"
            ],
            "metadata": {
                "azdata_cell_guid": "1dcd5e8e-1876-4848-82c6-5cd6269bdf67"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "azdata arc sql mi delete --name sqldemo02\n",
                "azdata arc sql mi delete --name sqldemo01\n",
                "azdata arc postgres server delete --name postgres01\n",
                "azdata logout"
            ],
            "metadata": {
                "azdata_cell_guid": "752e3b6f-a8cf-4a0e-a7ef-7e9eaff1c33d",
                "tags": []
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "**Everything deployed inside of Azure Arc Data Services is implemented as a Kubernetes Object...let's check out a few.** \n",
                "\n",
                "-   **Pods** - running container based applications inside the Kubernetes cluster\n",
                "-   **Services** - access endpoints to reach the applications running inside our cluster. There can be cluster internal (ClusterIP) and external Service endpoints (NodePort). Can be integrated with external LoadBalancers if needed.\n",
                "-   **DaemonSets** - ensure a Pod is running on each node in the cluster. Used by the Metrics data collector in Arc, in our case [Telegraf](https://github.com/influxdata/telegraf \"https://github.com/influxdata/telegraf\").\n",
                "-   **ReplicaSets** -  ensure a desired number of Pods are running in the cluster.  Primarily used by management software and web ports for logs and metrics (bootstrapper, control, controlwd, logsui, metricsui and mgmtproxy)\n",
                "-   **StatefulSets** - ensure a desired number of Pods are running in the cluster. But also provide stable naming on the Pods and cluster service discovery via Cluster DNS. Used by controldb, logsdb, metricsdb and also provisioned Managed Instances and Postgres instances. Enables scaling."
            ],
            "metadata": {
                "azdata_cell_guid": "aace495c-60c2-47af-8911-f42db0cc858f"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "kubectl get all --namespace arc"
            ],
            "metadata": {
                "azdata_cell_guid": "96adca0d-b964-4cbd-88cc-42e8e8de17c7",
                "tags": [
                    "hide_input"
                ]
            },
            "outputs": [],
            "execution_count": null
        }
    ]
}